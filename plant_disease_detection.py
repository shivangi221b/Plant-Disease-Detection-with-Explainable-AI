"""DLCV-PlantDiseaseDetection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pG9_FCzAksb8XRnlOJp3CrdS88xsD-Sd

# DATA PREPROCESSING AND BASE MODEL
"""

import sys
import os
import shutil
import subprocess
import json
from pathlib import Path
from typing import Optional, Dict, List, Tuple
import argparse
import re

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.optim.lr_scheduler import ReduceLROnPlateau
from torch.utils.data import DataLoader, random_split
import time

import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision import transforms

import numpy as np
from PIL import Image
import cv2
import matplotlib.pyplot as plt
from matplotlib.colors import Normalize
from matplotlib.cm import ScalarMappable
import seaborn as sns
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix
from tqdm import tqdm

from torchvision.datasets import ImageFolder
from torch.utils.data import random_split

# XAI imports
import requests
from lime import lime_image
from skimage.segmentation import felzenszwalb

# Custom segmentation function for LIME
from skimage.segmentation import felzenszwalb

from typing import Optional

import streamlit as st
status = st.empty()

import multiprocessing as mp

# ============================================================================
# DATASET - PlantVillageDataset
# ============================================================================

class PlantVillageDataset:
    """
    Handles PlantVillage dataset:
    - Downloading from GitHub
    - Doing data preprocessing & augmentation
    - Creating train/val/test splits
    - Creating PyTorch DataLoader
    """

    def __init__(self, data_dir='./data/plant_village', download=True):
        """
        Initialize PlantVillage dataset handler.

        Args:
            data_dir (str): Directory to store dataset
            download (bool): Whether to download if not exists
        """
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(parents=True, exist_ok=True)
        self.images_dir = self.data_dir / 'images'
        if download and not self._check_dataset_exists():
            self._download_dataset()

        self.disease_classes = self._get_disease_classes()
        self.num_classes = len(self.disease_classes)

    def _check_dataset_exists(self):
        # Check if dataset is already downloaded
        return self.images_dir.exists() and len(list(self.images_dir.glob('*'))) > 0

    def _download_dataset(self):
        status.write("Downloading PlantVillage dataset from GitHub...")

        repo_url = "https://github.com/spMohanty/PlantVillage-Dataset.git"
        temp_repo_dir = self.data_dir / 'temp_repo'

        try:
            # Clone repository
            result = subprocess.run(
                ['git', 'clone', '--depth', '1', repo_url, str(temp_repo_dir)],
                capture_output=True,
                timeout=600,
                text=True
            )

            # Check if git clone succeeded
            if result.returncode != 0:
                print(f"Git clone failed with return code {result.returncode}")
                print(f"STDERR: {result.stderr}")
                raise RuntimeError(f"Git clone failed: {result.stderr}")
            
            print("Repository cloned successfully")

            # Move images to data directory
            source_images = temp_repo_dir / 'raw' / 'color'
            if source_images.exists():
                for crop_dir in tqdm(source_images.iterdir(), desc="Organizing images"):
                    if crop_dir.is_dir():
                        dest = self.data_dir / 'images' / crop_dir.name
                        shutil.copytree(crop_dir, dest, dirs_exist_ok=True)

            # Clean up
            shutil.rmtree(temp_repo_dir)
            print("Dataset downloaded successfully!")

        except Exception as e:
          print("Alternative: Download from https://www.kaggle.com/abdallahalidev/plantvillage-dataset")

    def _get_disease_classes(self):
        # Get list of disease classes from dataset
        images_dir = self.data_dir / 'images'
        if not images_dir.exists():
            return []
        return sorted([d.name for d in images_dir.iterdir() if d.is_dir()])

    def get_transforms(self, augment=True, img_size=224):
        # Get data augmentation transforms
        normalize = transforms.Normalize(
            mean=[0.485, 0.456, 0.406],
            std=[0.229, 0.224, 0.225]
        )

        train_transform = transforms.Compose([
            transforms.RandomResizedCrop(img_size, scale=(0.8, 1.0)),
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.RandomVerticalFlip(p=0.3),
            transforms.RandomRotation(30),
            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),
            transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),
            transforms.ToTensor(),
            normalize
        ])

        val_transform = transforms.Compose([
            transforms.Resize(img_size + 32),
            transforms.CenterCrop(img_size),
            transforms.ToTensor(),
            normalize
        ])

        return {'train': train_transform, 'val': val_transform, 'test': val_transform}

    def create_dataloaders(self, batch_size=32, num_workers=0,
                          train_split=0.7, val_split=0.15):
        # Create train/val/test dataloaders
        images_dir = self.data_dir / 'images'
        transforms_dict = self.get_transforms(augment=True)

        print("Loading dataset...")
        full_dataset = ImageFolder(str(images_dir), transform=transforms_dict['train'])

        total_size = len(full_dataset)
        train_size = int(total_size * train_split)
        val_size = int(total_size * val_split)
        test_size = total_size - train_size - val_size

        train_dataset, val_dataset, test_dataset = random_split(
            full_dataset, [train_size, val_size, test_size]
        )

        train_dataset.transform = transforms_dict['train']
        val_dataset.transform = transforms_dict['val']
        test_dataset.transform = transforms_dict['test']

        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,
                                 num_workers=num_workers, pin_memory=True)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,
                               num_workers=num_workers, pin_memory=True)
        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,
                                num_workers=num_workers, pin_memory=True)


        print(f"Dataset splits created:")
        print(f"Train: {len(train_dataset)} images")
        print(f"Val:   {len(val_dataset)} images")
        print(f"Test:  {len(test_dataset)} images")
        print(f"Total classes: {self.num_classes}")

        return {
            'train': train_loader,
            'val': val_loader,
            'test': test_loader
        }, self.num_classes, self.disease_classes

def get_improved_transforms(img_size=224):
    # For improved augmentation, adding Gaussian blur for disease textures

    train_transform = transforms.Compose([
        transforms.RandomResizedCrop(img_size, scale=(0.75, 1.0)),
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.RandomRotation(30),
        transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),
        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),
        transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),  # NEW
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    val_transform = transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(img_size),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    return train_transform, val_transform

# ============================================================================
# BASELINE MODEL
# ============================================================================

class EfficientNetB0Classifier(nn.Module):
    # Tried using EfficientNetB0 classifier for plant disease detection

    def __init__(self, num_classes=38, pretrained=True, freeze_backbone=True):
        """
        Initialize EfficientNetB0 classifier.

        Args:
            num_classes (int): Number of disease classes
            pretrained (bool): Use ImageNet pre-trained weights
            freeze_backbone (bool): Freeze backbone during training
        """
        super().__init__()

        self.model = models.efficientnet_b0(
            weights=models.EfficientNet_B0_Weights.IMAGENET1K_V1 if pretrained else None
        )

        in_features = self.model.classifier[-1].in_features
        self.model.classifier[-1] = nn.Linear(in_features, num_classes)

        if freeze_backbone:
            for param in self.model.features.parameters():
                param.requires_grad = False

        self.num_classes = num_classes

    def forward(self, x):
        # Forward pass
        return self.model(x)

    def freeze_backbone(self):
        # Freeze backbone parameters
        for param in self.model.features.parameters():
            param.requires_grad = False

    def unfreeze_backbone(self):
        # Unfreeze backbone parameters
        for param in self.model.features.parameters():
            param.requires_grad = True

    def get_trainable_params(self):
        # Get count of trainable parameters
        return sum(p.numel() for p in self.parameters() if p.requires_grad)

class EfficientNetB3Classifier(nn.Module):
    # EfficientNetB3 has better accuracy than B0, and it also has a reasonable training time

    def __init__(self, num_classes=38, pretrained=True):
        super().__init__()

        self.model = models.efficientnet_b3(
            weights=models.EfficientNet_B3_Weights.IMAGENET1K_V1 if pretrained else None
        )

        # Replace classifier with dropout
        in_features = self.model.classifier[1].in_features
        self.model.classifier = nn.Sequential(
            nn.Dropout(0.3),
            nn.Linear(in_features, num_classes)
        )

        self.num_classes = num_classes

    def forward(self, x):
        return self.model(x)

    def freeze_backbone(self):
        for param in self.model.features.parameters():
            param.requires_grad = False

    def unfreeze_backbone(self):
        for param in self.model.features.parameters():
            param.requires_grad = True

# ============================================================================
# TRAINING UTILITY FUNCTIONS
# ============================================================================

class Trainer:
    def __init__(self, model, device='cuda', save_dir='./models'):
        # Initialize trainer
        self.model = model.to(device)
        self.device = device
        self.save_dir = Path(save_dir)
        self.save_dir.mkdir(exist_ok=True)
        self.best_val_acc = 0

    def train_epoch(self, train_loader, optimizer, criterion):
        # Train for one epoch
        self.model.train()
        total_loss = 0

        for batch_idx, (images, labels) in enumerate(train_loader):
            images = images.to(self.device)
            labels = labels.to(self.device)

            optimizer.zero_grad()
            outputs = self.model(images)
            loss = criterion(outputs, labels)

            loss.backward()
            optimizer.step()

            total_loss += loss.item()

            if (batch_idx + 1) % 10 == 0:
                print(f"Batch {batch_idx+1}, Loss: {loss.item():.4f}")

        return total_loss / len(train_loader)

    def validate(self, val_loader, criterion):
        # Validate model
        self.model.eval()
        total_loss = 0
        correct = 0
        total = 0

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(self.device)
                labels = labels.to(self.device)

                outputs = self.model(images)
                loss = criterion(outputs, labels)

                total_loss += loss.item()
                _, predicted = outputs.max(1)
                correct += predicted.eq(labels).sum().item()
                total += labels.size(0)

        val_loss = total_loss / len(val_loader)
        val_acc = 100. * correct / total

        return val_loss, val_acc

    def fit(self, train_loader, val_loader, epochs=10, lr=1e-4):
        # Train model for multiple epochs
        criterion = nn.CrossEntropyLoss()
        optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)
        scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=3)

        for epoch in range(epochs):
            print(f"\nEpoch {epoch+1}/{epochs}")

            train_loss = self.train_epoch(train_loader, optimizer, criterion)
            print(f"Train Loss: {train_loss:.4f}")

            val_loss, val_acc = self.validate(val_loader, criterion)
            print(f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%")

            if val_acc > self.best_val_acc:
                self.best_val_acc = val_acc
                torch.save(self.model.state_dict(),
                          self.save_dir / 'best_model.pth')
                print(f"Saved best model (Acc: {val_acc:.2f}%)")

            scheduler.step(val_acc)


def preprocess_image(image_pil, img_size=224):
    # Preprocess PIL image for inference
    image_pil = image_pil.resize((img_size, img_size))
    image_array = np.array(image_pil) / 255.0

    image_array = (image_array - np.array([0.485, 0.456, 0.406])) / np.array([0.229, 0.224, 0.225])
    image_tensor = torch.from_numpy(image_array).permute(2, 0, 1).float()

    return image_tensor.unsqueeze(0)


def inference(model, image_tensor, device):
    # Run model inference
    model.eval()
    image_tensor = image_tensor.to(device)

    with torch.no_grad():
        logits = model(image_tensor)
        probs = torch.softmax(logits, dim=1)[0]

    confidence, pred_idx = torch.max(probs, dim=0)

    return pred_idx.item(), confidence.item(), probs.cpu().numpy()

# ============================================================================
# DATA PREPROCESSING
# ============================================================================

def run_data_preprocessing():
    # Download and prepare dataset
    print("Started Data Preprocessing...")

    dataset = PlantVillageDataset(download=True) 
    dataloaders, num_classes, disease_classes = dataset.create_dataloaders()

    # Visualize
    Path('visualizations').mkdir(exist_ok=True)

    fig, axes = plt.subplots(4, 4, figsize=(12, 12))
    axes = axes.flatten()

    normalize_stats = ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])

    idx = 0
    for images, labels in dataloaders['train']:
        for i, (image, label) in enumerate(zip(images[:16], labels[:16])):
            image = image.clone()
            for ch in range(3):
                image[ch] = image[ch] * normalize_stats[1][ch] + normalize_stats[0][ch]
            image = torch.clamp(image, 0, 1)

            axes[idx].imshow(image.permute(1, 2, 0).numpy())
            axes[idx].set_title(disease_classes[label.item()], fontsize=8)
            axes[idx].axis('off')
            idx += 1
        break

    plt.tight_layout()
    plt.savefig('visualizations/sample_images.png', dpi=100, bbox_inches='tight')
    print("Saved sample_images.png")

run_data_preprocessing()


# ============================================================================
# TRAINING BASELINE MODEL
# ============================================================================

def run_training():
    print("Baseline Model Training Started...")

    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"Device: {device}")

    dataset = PlantVillageDataset(download=False)
    dataloaders, num_classes, disease_classes = dataset.create_dataloaders()

    model = EfficientNetB0Classifier(num_classes=num_classes, pretrained=True, freeze_backbone=True)
    print(f"Trainable parameters: {model.get_trainable_params():,}")

    trainer = Trainer(model, device=device)
    trainer.fit(dataloaders['train'], dataloaders['val'], epochs=10, lr=1e-4)

    print("Baseline Model Training Complete!")


run_training()

# ============================================================================
# FINE TUNING BASELINE MODEL
# ============================================================================

def run_fine_tuning():
    """
    Enhanced fine-tuning with 2-phase training.
    Phase 1: Train classifier only (5 epochs)
    Phase 2: Fine-tune full network (8 epochs)
    """
    print("Fine-Tuning Baseline Model...")

    device = 'cuda' if torch.cuda.is_available() else 'cpu' 
    print(f"Device: {device}")

    # Load dataset
    dataset = PlantVillageDataset(download=False)

    # Use improved transforms
    train_transform, val_transform = get_improved_transforms()

    images_dir = dataset.data_dir / 'images'
    full_dataset = ImageFolder(str(images_dir))

    total_size = len(full_dataset)
    train_size = int(0.7 * total_size)
    val_size = int(0.15 * total_size)
    test_size = total_size - train_size - val_size

    train_dataset, val_dataset, test_dataset = random_split(
        full_dataset, [train_size, val_size, test_size]
    )

    train_dataset.dataset.transform = train_transform
    val_dataset.dataset.transform = val_transform
    test_dataset.dataset.transform = val_transform

    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0, pin_memory=True)
    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0, pin_memory=True)
    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0, pin_memory=True)

    num_classes = len(full_dataset.classes)
    print(f"Classes: {num_classes}, Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}")

    # Use improved model
    model = EfficientNetB3Classifier(num_classes=num_classes, pretrained=True)
    model = model.to(device)

    # Label smoothing for better generalization
    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)

    save_dir = Path('./models/fine_tuned')
    save_dir.mkdir(exist_ok=True, parents=True)
    best_val_acc = 0

    # Train Classifier Only
    print("PHASE 1: Training Classifier (Frozen Backbone) - 5 epochs")

    model.freeze_backbone()
    optimizer = torch.optim.AdamW(
        filter(lambda p: p.requires_grad, model.parameters()),
        lr=1e-3, weight_decay=0.01
    )
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=5)

    for epoch in range(5):
        model.train()
        train_loss = 0
        correct = 0
        total = 0

        for images, labels in tqdm(train_loader, desc=f"Epoch {epoch+1}/5"):
            images, labels = images.to(device), labels.to(device)

            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            _, predicted = outputs.max(1)
            correct += predicted.eq(labels).sum().item()
            total += labels.size(0)

        train_acc = 100. * correct / total

        # Validate
        model.eval()
        val_loss = 0
        correct = 0
        total = 0
        with torch.no_grad():
            for images, labels in val_loader:
                images, labels = images.to(device), labels.to(device)
                outputs = model(images)
                loss = criterion(outputs, labels)
                val_loss += loss.item()
                _, predicted = outputs.max(1)
                correct += predicted.eq(labels).sum().item()
                total += labels.size(0)

        val_acc = 100. * correct / total
        scheduler.step()

        print(f"Epoch {epoch+1}: Train Acc={train_acc:.2f}%, Val Acc={val_acc:.2f}%")

        if val_acc > best_val_acc:
            best_val_acc = val_acc
            torch.save(model.state_dict(), save_dir / 'phase1_best.pth')
            print(f"Saved (Val Acc: {val_acc:.2f}%)")

    # Fine-tune Full Network
    print("PHASE 2: Full Fine-tuning (Low LR) - 8 epochs")

    model.load_state_dict(torch.load(save_dir / 'phase1_best.pth'))
    model.unfreeze_backbone()

    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, weight_decay=0.01)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=8)

    for epoch in range(8):
        model.train()
        train_loss = 0
        correct = 0
        total = 0

        for images, labels in tqdm(train_loader, desc=f"Epoch {epoch+1}/8"):
            images, labels = images.to(device), labels.to(device)

            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            _, predicted = outputs.max(1)
            correct += predicted.eq(labels).sum().item()
            total += labels.size(0)

        train_acc = 100. * correct / total

        # Validate
        model.eval()
        val_loss = 0
        correct = 0
        total = 0
        with torch.no_grad():
            for images, labels in val_loader:
                images, labels = images.to(device), labels.to(device)
                outputs = model(images)
                loss = criterion(outputs, labels)
                val_loss += loss.item()
                _, predicted = outputs.max(1)
                correct += predicted.eq(labels).sum().item()
                total += labels.size(0)

        val_acc = 100. * correct / total
        scheduler.step()

        print(f"Epoch {epoch+1}: Train Acc={train_acc:.2f}%, Val Acc={val_acc:.2f}%")

        if val_acc > best_val_acc:
            best_val_acc = val_acc
            torch.save(model.state_dict(), save_dir / 'best_model.pth')
            print(f"Saved best model (Val Acc: {val_acc:.2f}%)")

    # Final Test Evaluation
    print("Evaluating Model...")

    model.load_state_dict(torch.load(save_dir / 'best_model.pth'))
    model.eval()

    correct = 0
    total = 0
    with torch.no_grad():
        for images, labels in tqdm(test_loader, desc="Testing"):
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, predicted = outputs.max(1)
            correct += predicted.eq(labels).sum().item()
            total += labels.size(0)

    test_acc = 100. * correct / total

    print(f"\nRESULTS:")
    print(f"Best Validation Accuracy: {best_val_acc:.2f}%")
    print(f"Final Test Accuracy: {test_acc:.2f}%")
    print(f"Model saved to: {save_dir / 'best_model.pth'}")
    print("\nFine-tuning complete!")

run_fine_tuning()

# ============================================================================
# EXPLAINABILITY
# ============================================================================

class GradCAMExplainer:
    """
    Gradient-weighted Class Activation Mapping (Grad-CAM).
    Shows which image regions are important for predictions.
    """

    def __init__(self, model, target_layer):
        # Initialize Grad-CAM explainer
        self.model = model
        self.target_layer = target_layer
        self.gradients = None
        self.activations = None
        self._register_hooks()

    def _register_hooks(self):
        # Register forward and backward hooks
        def forward_hook(module, input, output):
            self.activations = output.detach()

        def backward_hook(module, grad_input, grad_output):
            self.gradients = grad_output[0].detach()

        if isinstance(self.target_layer, str):
            target = dict(self.model.named_modules())[self.target_layer]
        else:
            target = self.target_layer

        target.register_forward_hook(forward_hook)
        target.register_backward_hook(backward_hook)

    def generate_cam(self, input_tensor, class_idx=None):
        """
        Generate Grad-CAM visualization.

        Returns:
            np.ndarray: CAM heatmap (0-1) matching input size
        """
        self.model.eval()
        logits = self.model(input_tensor)

        if class_idx is None:
            class_idx = logits.argmax(dim=1).item()

        self.model.zero_grad()
        one_hot = torch.zeros_like(logits)
        one_hot[0, class_idx] = 1.0
        logits.backward(gradient=one_hot)

        gradients = self.gradients[0]
        activations = self.activations[0]

        weights = gradients.mean(dim=(1, 2))
        cam = torch.zeros(activations.shape[1:], device=activations.device)
        for i in range(len(weights)):
            cam += weights[i] * activations[i]

        cam = torch.relu(cam)
        cam_min = cam.min()
        cam_max = cam.max()
        if cam_max > cam_min:
            cam = (cam - cam_min) / (cam_max - cam_min)
        else:
            cam = torch.zeros_like(cam)

        return cam.cpu().numpy()

    def visualize_cam(self, input_image, cam, alpha=0.4):
        # Overlay Grad-CAM heatmap on original image
        h, w = input_image.shape[:2]
        cam_resized = cv2.resize(cam, (w, h))

        heatmap = cv2.applyColorMap((cam_resized * 255).astype(np.uint8), cv2.COLORMAP_JET)
        overlay = cv2.addWeighted(input_image, 1 - alpha, heatmap, alpha, 0)
        return overlay

class LIMEExplainer:
    # LIME explainer for image classification

    def __init__(self, model, device='cpu', num_samples=100):
        self.model = model
        self.device = device
        self.num_samples = num_samples

    def _model_predict_fn(self, images):
        # Wrapper function for LIME to call model
        self.model.eval()

        # images shape: (N, H, W, C), values 0-255
        images_tensor = torch.from_numpy(images).permute(0, 3, 1, 2).float()
        images_tensor = images_tensor / 255.0  # Normalize to 0-1

        # Apply ImageNet normalization
        mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)
        std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)
        images_tensor = (images_tensor - mean) / std

        images_tensor = images_tensor.to(self.device)

        with torch.no_grad():
            logits = self.model(images_tensor)
            probs = F.softmax(logits, dim=1)

        return probs.cpu().numpy()

    @staticmethod
    def my_segmentation(image, *args, **kwargs):
        # Ensure float64 for felzenszwalb
        if image.dtype != np.float64:
            image = image.astype(np.float64)
        return felzenszwalb(image, scale=70, sigma=0.25, min_size=40)

    def explain(self, image_array, class_idx=None):
        """
        Generate LIME explanation.

        Args:
            image_array: numpy array (H, W, 3), uint8 (0-255)
            class_idx: class to explain
        """

        # Ensure correct format
        if not isinstance(image_array, np.ndarray):
            image_array = np.array(image_array)
        if image_array.dtype != np.uint8:
            if image_array.max() <= 1.0:
                image_array = (image_array * 255).astype(np.uint8)
            else:
                image_array = image_array.astype(np.uint8)

        explainer = lime_image.LimeImageExplainer()

        explanation = explainer.explain_instance(
            image_array,
            self._model_predict_fn,
            top_labels=1,
            num_samples=self.num_samples,
            segmentation_fn=self.my_segmentation
        )

        return explanation

    def visualize_explanation(self, image_array, explanation, num_features=10, hide_rest=True):
        """
        Visualize LIME explanation with better contrast.
        Always returns uint8 array for compatibility with cv2.imwrite.
        """
        temp, mask = explanation.get_image_and_mask(
            explanation.top_labels[0],
            positive_only=True,
            num_features=num_features,
            hide_rest=hide_rest
        )

        # Ensure uint8 output
        if temp.dtype != np.uint8:
            if temp.max() <= 1.0:
                temp = (temp * 255).astype(np.uint8)
            else:
                temp = np.clip(temp, 0, 255).astype(np.uint8)

        return temp

class ExplanationToText:
    # Convert visual explanations to natural language text

    @staticmethod
    def gradcam_to_text(cam, disease_name, threshold=0.5):
        """
        Convert Grad-CAM heatmap to textual description.

        Returns:
            str: Natural language explanation
        """
        h, w = cam.shape

        significant_mask = cam > threshold
        significant_ratio = significant_mask.sum() / (h * w)

        # Analyze regions
        top_region = significant_mask[:h//3].mean()
        bottom_region = significant_mask[2*h//3:].mean()
        left_region = significant_mask[:, :w//3].mean()
        right_region = significant_mask[:, 2*w//3:].mean()
        center_region = significant_mask[h//3:2*h//3, w//3:2*w//3].mean()

        regions = []
        if top_region > 0.3:
            regions.append("top")
        if bottom_region > 0.3:
            regions.append("bottom")
        if left_region > 0.3:
            regions.append("left")
        if right_region > 0.3:
            regions.append("right")
        if center_region > 0.3:
            regions.append("center")

        if not regions:
            regions = ["throughout the leaf"]
        else:
            regions = [f"{r} region" for r in regions]

        region_text = " and ".join(regions)
        severity = "severe" if significant_ratio > 0.5 else "mild to moderate"

        explanation = (
            f"The model detected {disease_name} disease. "
            f"The disease symptoms are concentrated in the {region_text} "
            f"with {severity} severity ({significant_ratio*100:.1f}% of leaf affected). "
            f"The highlighted regions show where the model focused to make this prediction."
        )

        return explanation

    @staticmethod
    def lime_to_text(explanation, disease_name, top_features=5):
        # Convert LIME explanation to textual description
        weights = dict(explanation.local_exp[explanation.top_labels[0]])
        top_weights = sorted(weights.items(), key=lambda x: abs(x[1]), reverse=True)[:top_features]

        importance_scores = [w for _, w in top_weights]
        avg_importance = np.mean(importance_scores)

        confidence_level = "high confidence" if avg_importance > 0.3 else "moderate confidence"

        explanation_text = (
            f"LIME analysis identified {top_features} key image regions that strongly influenced "
            f"the {disease_name} prediction with {confidence_level}. "
            f"These superpixels are the most informative for the model's decision."
        )

        return explanation_text

def run_xai_comparison():
    """
    Compare GRAD-CAM and LIME visually and textually in the notebook.
    For each sampled image:
        - Show original image
        - Show Grad-CAM overlay
        - Show LIME superpixel mask
        - Print explanations
        - Save all images to disk
    """
    import matplotlib.pyplot as plt

    print("\n" + "="*80)
    print("XAI COMPARISON")
    print("="*80)

    device = 'cuda' if torch.cuda.is_available() else 'cpu'

    dataset = PlantVillageDataset(download=False)
    dataloaders, num_classes, disease_classes = dataset.create_dataloaders()

    model = EfficientNetB3Classifier(num_classes=num_classes)
    model.load_state_dict(torch.load('best_model.pth', map_location=device))
    model = model.to(device).eval()

    gradcam = GradCAMExplainer(model, model.model.features[-1])
    lime = LIMEExplainer(model, device=device)

    vis_dir = Path("visualizations/xai_comparison")
    vis_dir.mkdir(exist_ok=True, parents=True)

    print("Generating explanations and images...")
    num_samples = 5
    count = 0
    for batch_idx, (images, labels) in enumerate(dataloaders['test']):
        for i, (image, label) in enumerate(zip(images, labels)):
            if count >= num_samples:
                break
            image_tensor = image.unsqueeze(0).to(device)

            # Denormalize and convert to uint8 for display and saving
            image_np = image.permute(1, 2, 0).cpu().numpy()
            mean = np.array([0.485, 0.456, 0.406])
            std = np.array([0.229, 0.224, 0.225])
            image_vis = (image_np * std + mean)
            image_vis = np.clip(image_vis, 0, 1)
            image_vis_rgb = (image_vis * 255).astype(np.uint8)

            disease = disease_classes[label.item()]

            # Grad-CAM
            cam = gradcam.generate_cam(image_tensor, class_idx=label.item())
            cam_overlay = gradcam.visualize_cam(image_vis_rgb, cam)
            cam_path = vis_dir / f"sample{count+1}_gradcam_{disease.replace('/', '-')}.png"
            cv2.imwrite(str(cam_path), cv2.cvtColor(cam_overlay, cv2.COLOR_RGB2BGR))
            xai_text = ExplanationToText.gradcam_to_text(cam, disease)

            # LIME
            lime_viz = None
            lime_text = None

            explanation = lime.explain(image_vis_rgb, class_idx=label.item())
            # Use hide_rest=True to show ONLY important regions
            lime_viz = lime.visualize_explanation(image_vis_rgb, explanation, num_features=10, hide_rest=True)
            lime_path = vis_dir / f"sample{count+1}_lime_{disease.replace('/', '-')}.png"
            cv2.imwrite(str(lime_path), cv2.cvtColor(lime_viz, cv2.COLOR_RGB2BGR))
            lime_text = ExplanationToText.lime_to_text(explanation, disease)

            # DISPLAY IN NOTEBOOK
            fig, axs = plt.subplots(1, 3 if lime_viz is not None else 2, figsize=(14, 5))
            axs[0].imshow(image_vis_rgb)
            axs[0].set_title('Original')
            axs[0].axis('off')

            axs[1].imshow(cam_overlay)
            axs[1].set_title('Grad-CAM Overlay')
            axs[1].axis('off')

            if lime_viz is not None:
                axs[2].imshow(lime_viz)
                axs[2].set_title('LIME Mask')
                axs[2].axis('off')

            plt.suptitle(f"{disease}", fontsize=14)
            plt.show()

            print(f"\nSample {count+1}: {disease}")
            print(f"Grad-CAM: {xai_text}")
            if lime_text:
                print(f"LIME: {lime_text}")

            print(f"(Images saved to {vis_dir})\n" + "-"*30)
            count += 1
            if count >= num_samples:
                break

run_xai_comparison()          

# ==========================================
# LLM UTILITY FUNCTIONS
# ==========================================

# ------------------------------------------
# LLAMA2
# ------------------------------------------

class Llama2Handler:
    # Llama2 handler (self-hosted via Ollama)

    def __init__(self, base_url: str = "http://localhost:11434"):
        # Initialize Llama2 handler
        self.base_url = base_url
        self.model = "llama2"

        try:
            requests.get(f"{self.base_url}/api/tags", timeout=2)
            self.available = True
        except:
            print("Ollama not running. Start with: ollama serve")
            self.available = False

    def generate_treatment(self, disease_name: str, xai_explanation: str) -> str:
        # Generate treatment using Llama2
        if not self.available:
            return "Ollama not running. Start with: ollama serve && ollama pull llama2"

        prompt = f"""You are an agricultural disease expert. Given a plant disease, provide treatment recommendations.

        Disease: {disease_name}

        Explanation: {xai_explanation}

        Provide:
        1. Immediate actions
        2. Treatment recommendations
        3. Dosage information
        4. Prevention tips
        5. Recovery timeline

        Be concise and practical."""

        try:
            response = requests.post(
                f"{self.base_url}/api/generate",
                json={"model": self.model, "prompt": prompt, "stream": False},
                timeout=300
            )
            result = response.json()
            return result.get('response', 'No response generated')
        except Exception as e:
            return f"Error: {str(e)}"

test_disease = "Tomato Early Blight"
test_explanation = (
    "The model detected Tomato Early Blight disease. "
    "The disease symptoms are concentrated in the bottom region "
    "with severe severity (65% of leaf affected)."
)

# ------------------------------------------
# MISTRAL
# ------------------------------------------

class MistralHandler:
    # Mistral handler (self-hosted via Ollama)

    def __init__(self, base_url: str = "http://localhost:11434"):
        #Initialize Mistral handler
        self.base_url = base_url
        # This is the default general-purpose Mistral model name in Ollama
        self.model = "mistral"

        try:
            # Check if Ollama is running
            requests.get(f"{self.base_url}/api/tags", timeout=2)
            self.available = True
        except Exception:
            print("Ollama not running. Start with: ollama serve && ollama pull mistral")
            self.available = False

    def generate_treatment(self, disease_name: str, xai_explanation: str) -> str:
        #Generate treatment using Mistral
        if not self.available:
            return "Ollama not running. Start with: ollama serve && ollama pull mistral"

        prompt = f"""You are an agricultural disease expert. Given a plant disease, provide treatment recommendations.

Disease Detected: {disease_name}

Analysis: {xai_explanation}

Provide actionable treatment recommendations covering:
- Immediate steps
- Treatment products and dosages
- Application schedule
- Prevention measures
- Expected recovery period

Be specific and practical."""

        try:
            response = requests.post(
                f"{self.base_url}/api/generate",
                json={
                    "model": self.model,
                    "prompt": prompt,
                    "stream": False
                },
                timeout=300  # increased to 5 minutes, like Llama2Handler
            )
            result = response.json()
            return result.get("response", "No response generated")
        except Exception as e:
            return f"Error: {str(e)}"

# ------------------------------------------
# GEMINI
# ------------------------------------------
class GeminiHandler:
    #Google Gemini API handler

    def __init__(self, api_key: Optional[str] = None):
        # Initialize Gemini handler
        self.api_key = api_key or os.environ.get('GEMINI_API_KEY')
        if not self.api_key:
            print("Gemini API key not found.")
            print("Get free key: https://aistudio.google.com/app/apikey")
            self.available = False
            return

        try:
            import google.generativeai as genai
            genai.configure(api_key=self.api_key)

            #Using the newest free model
            self.model = genai.GenerativeModel('models/gemini-2.5-flash')
            self.available = True
            print("Gemini 2.5 Flash initialized successfully!")

        except ImportError:
            print("Install google-generativeai: pip install google-generativeai")
            self.available = False
        except Exception as e:
            print(f"Gemini initialization error: {e}")
            self.available = False

    def generate_treatment(self, disease_name: str, xai_explanation: str) -> str:
        # Generate treatment using Gemini
        if not self.available:
            return "Gemini not available. Check API key and installation."

        prompt = f"""You are an expert agricultural disease specialist. Based on the following plant disease diagnosis,
provide practical treatment recommendations.

DISEASE DETECTED: {disease_name}

EXPLAINABILITY ANALYSIS:
{xai_explanation}

Please provide:
1. **Immediate Actions**: What the farmer should do right now
2. **Treatment Options**: Specific fungicides/pesticides/biological controls
3. **Dosage & Application**: How much to use and how often
4. **Prevention Tips**: How to prevent future outbreaks
5. **Timeline**: Expected recovery time

Keep the response concise, practical, and actionable for farmers."""

        try:
            response = self.model.generate_content(prompt)
            return response.text
        except Exception as e:
            return f"Error: {str(e)}"

os.environ['GEMINI_API_KEY'] = 'Axxxxxxxxxxxxx' #Use your Gemini API key here

# ------------------------------------------
# 3-LLM COMPARISON (Gemini 2.5 + Llama2 + Mistral)
# ------------------------------------------

def compare_llms():
    # Compare the 3 LLMs: Gemini 2.5 Flash, Llama2 and Mistral

    # Sample test cases
    test_cases = [
        {
            "disease": "Tomato Early Blight",
            "explanation": "The model detected Tomato Early Blight disease. Disease symptoms are concentrated in the bottom and center regions with severe severity (68% of leaf affected)."
        },
        {
            "disease": "Potato Late Blight",
            "explanation": "The model detected Potato Late Blight disease. Disease symptoms are concentrated in the top and left regions with moderate severity (42% of leaf affected)."
        },
        {
            "disease": "Apple Scab",
            "explanation": "The model detected Apple Scab disease. Disease symptoms are throughout the leaf with mild severity (35% of leaf affected)."
        }
    ]

    # Initialize all 3 LLM handlers
    handlers = {
        "Gemini 2.5 Flash": GeminiHandler(),
        "Llama2": Llama2Handler(),
        "Mistral": MistralHandler()
    }

    results = {}

    for llm_name, handler in handlers.items():
        print(f"Testing {llm_name}")

        if not handler.available:
            print(f"{llm_name} not available")
            results[llm_name] = {"available": False}
            continue

        llm_results = {
            "available": True,
            "responses": [],
            "avg_time": 0,
            "total_time": 0
        }

        times = []

        for i, test_case in enumerate(test_cases, 1):
            print(f"\n Test Case {i}: {test_case['disease']}")

            start_time = time.time()
            response = handler.generate_treatment(
                test_case['disease'],
                test_case['explanation']
            )
            elapsed = time.time() - start_time
            times.append(elapsed)

            print(f"Response time: {elapsed:.2f}s")
            print(f"Response preview: {response[:200]}...")

            llm_results["responses"].append({
                "disease": test_case['disease'],
                "response": response,
                "time": elapsed,
                "length": len(response)
            })

        llm_results["avg_time"] = sum(times) / len(times)
        llm_results["total_time"] = sum(times)
        results[llm_name] = llm_results

    # Comparison Summary
    print("COMPARISON SUMMARY")

    print(f"\n{'LLM':<25} {'Available':<12} {'Avg Time':<12} {'Total Time':<12}")

    for llm_name, data in results.items():
        if data["available"]:
            print(f"{llm_name:<25} {'Yes':<12} {data['avg_time']:>8.2f}s {data['total_time']:>11.2f}s")
        else:
            print(f"{llm_name:<25} {'No':<12} {'N/A':<12} {'N/A':<12}")

    # Find the fastest LLM Model
    available_llms = {k: v for k, v in results.items() if v["available"]}
    if available_llms:
        fastest = min(available_llms.items(), key=lambda x: x[1]["avg_time"])
        print(f"\nFastest LLM: {fastest[0]} ({fastest[1]['avg_time']:.2f}s avg)")

    # Save detailed results
    output = {}
    for llm_name, data in results.items():
        if data["available"]:
            output[llm_name] = {
                "available": True,
                "avg_response_time_seconds": round(data["avg_time"], 2),
                "total_time_seconds": round(data["total_time"], 2),
                "num_tests": len(data["responses"]),
                "avg_response_length_chars": sum(r["length"] for r in data["responses"]) // len(data["responses"]),
                "sample_responses": [
                    {
                        "disease": r["disease"],
                        "response_preview": r["response"][:300] + "...",
                        "time_seconds": round(r["time"], 2)
                    }
                    for r in data["responses"]
                ]
            }
        else:
            output[llm_name] = {"available": False}

    with open('llm_comparison_3_free.json', 'w') as f:
        json.dump(output, f, indent=2)

    print("\nDetailed results saved to: llm_comparison_3_free.json")

    return results

comparison_results = compare_llms()

# ==========================================
# STREAMLIT UI 
# ==========================================

def clean_class_name(raw: str) -> str:
    # Replace triple or more underscores with a space + dash + space
    name = re.sub(r"_{3,}", " - ", raw)

    # Replace remaining single or double underscores with spaces
    name = re.sub(r"_{1,2}", " ", name)

    # Replace leftover multiple spaces with single space
    name = re.sub(r"\s+", " ", name)

    # Strip leading/trailing spaces
    name = name.strip()

    # Capitalize each word properly
    name = " ".join(word.capitalize() for word in name.split(" "))

    return name

def create_streamlit():
    st.set_page_config(
        page_title="Plant Disease Detector",
        page_icon="üå±",
        layout="wide"
    )

    st.markdown("""
        <style>
        /* Global padding */
        .block-container {
            padding-top: 2rem;
            padding-bottom: 2rem;
        }

        /* title */
        .main-header {
            font-size: 3.5em;
            font-weight: 900;
            text-align: center;
            letter-spacing: -1px;
            color: #0db85d;
            margin-bottom: 0.5em;
        }

        /* Section headers */
        .section-title {
            font-size: 1.8em;
            font-weight: 700;
            margin-top: 0.2em;
            color: #1e8449;
        }

        /* Card-style containers */
        .card {
            background-color: #f8fff8;
            padding: 0.1rem;
            border-radius: 14px;
            border: 1px solid #d5ecd5;
            box-shadow: 0px 2px 4px rgba(0,0,0,0.05);
            margin-bottom: 0.5rem;
        }

        /* Improve expander */
        .streamlit-expanderHeader {
            font-size: 1.1em !important;
            font-weight: 700 !important;
            color: #1e8449 !important;
        }

        </style>
    """, unsafe_allow_html=True)

    st.markdown('<div class="main-header">üå± Plant Disease Detection System</div>', unsafe_allow_html=True)
    
    st.markdown("""
    <div style='text-align:center; font-size:1.6em; font-weight:bold; margin-bottom:0.3rem'>
        Advanced AI-powered plant disease diagnosis with explainability and treatment recommendations
    </div>

    <div style='font-size:1.3em;'>
        Upload a leaf image to:
        <ul>
            <li>üîç Detect leaf's disease using deep learning (EfficientNetB3)</li>
            <li>üí° Understand predictions with GRAD-CAM and LIME explanations</li>
            <li>üíä Get AI-powered treatment recommendations from local LLM (Gemini)</li> 
        </ul>
    </div>
""", unsafe_allow_html=True)
    st.markdown("<div class='card'>", unsafe_allow_html=True)

    # ---------------------------------------------------
    # Sidebar
    # ---------------------------------------------------
    st.sidebar.header("‚öôÔ∏è Settings")

    llm_choice = st.sidebar.multiselect(
        "LLMs: ",
        ["Gemini", "Mistral", "Llama"],
        default="Gemini" 
    )

    # ---------------------------------------------------
    # Load Model + Explainers
    # ---------------------------------------------------
    @st.cache_resource
    def load_models():
        device = 'cuda' if torch.cuda.is_available() else 'cpu'

        dataset = PlantVillageDataset(download=True)
        num_classes = dataset.num_classes
        disease_classes = dataset.disease_classes

        model = EfficientNetB3Classifier(num_classes=num_classes, pretrained=False)

        if Path("best_model.pth").exists():
            model.load_state_dict(torch.load("best_model.pth", map_location=device))
        else:
            print("best_model.pth not found, using random weights.")

        model = model.to(device).eval()

        gradcam = GradCAMExplainer(model, model.model.features[-1])
        lime = LIMEExplainer(model, device=device, num_samples=50)

        return model, device, disease_classes, gradcam, lime

    try:
        model, device, disease_classes, gradcam, lime = load_models()
    except Exception as e:
        st.error(f"Model loading error: {e}")
        return

    # ---------------------------------------------------
    # Main UI Layout
    # ---------------------------------------------------
    st.markdown("<div class='section-title'>üì∏ Upload Leaf Image</div>", unsafe_allow_html=True)

    left, right = st.columns([1,1])

    with left:
        with st.container():
            image_file = st.file_uploader(
                "Upload your image here...",
                type=["jpg", "jpeg", "png"]
            )
    with right:
        if image_file:
            with st.spinner("Uploading image..."):
                image_pil = Image.open(image_file).convert("RGB")
                st.image(image_pil, caption="Uploaded Leaf Image", use_container_width=True)

    # ---------------------------------------------------
    # Prediction Section
    # ---------------------------------------------------
    if image_file:
        st.markdown("<div class='card'>", unsafe_allow_html=True)
        st.markdown("<div class='section-title'>üîç Disease Detection</div>", unsafe_allow_html=True)

        with st.spinner("Analyzing image..."):
            image_tensor = preprocess_image(image_pil)
            pred_idx, confidence, all_probs = inference(model, image_tensor, device)
            disease_name = clean_class_name(disease_classes[pred_idx])
            
            st.success(f"### üåø Detected: **{disease_name}**")
            st.markdown(
                f"<p style='font-size:22px; font-weight:600;'>Confidence: {confidence*100:.1f}%</p>",
                unsafe_allow_html=True
            )

            st.progress(float(confidence))
            with st.expander("üìä View Top Predictions"):
                top_5 = np.argsort(all_probs)[-5:][::-1]
                for i, idx in enumerate(top_5, 1):
                    st.write(f"{i}. **{disease_classes[idx]}** ‚Äî {all_probs[idx]*100:.2f}%")

        st.markdown("</div>", unsafe_allow_html=True)

    # ---------------------------------------------------
    # Explainability Section
    # ---------------------------------------------------
    if image_file:
        st.markdown("<div class='card'>", unsafe_allow_html=True)
        st.markdown("<div class='section-title'>üí° Explainability Analysis</div>", unsafe_allow_html=True)
        
        img_np = np.array(image_pil)
        xai_method = "GRAD-CAM"

        # GRAD-CAM
        if "GRAD-CAM" in xai_method:
            st.subheader("üî• GRAD-CAM")
            cam = gradcam.generate_cam(image_tensor.to(device), class_idx=pred_idx)
            overlay = gradcam.visualize_cam(img_np, cam)

            grad_col1, grad_col2 = st.columns([1.2, 1])
            with grad_col1:
                st.image(overlay, caption="Grad-CAM Heatmap", use_container_width=True)
            with grad_col2:
                explanation_text = ExplanationToText.gradcam_to_text(cam, disease_name)

                st.markdown(
                    f"""
                    <div style="
                        background-color: #f0fdf4;
                        border-left: 6px solid #16a34a;
                        padding: 15px;
                        border-radius: 8px;
                        font-size: 18px;
                        line-height: 1.5;
                    ">
                        <strong style="color:#15803d; font-size:20px;">üß† Explanation</strong><br><br>
                        {explanation_text}
                    </div>
                    """,
                    unsafe_allow_html=True
                )
        # LIME
        if "LIME" in xai_method:
            st.subheader("LIME Explanation")
            with st.spinner("Generating LIME explanation..."):
                lime_exp = lime.explain(img_np, class_idx=pred_idx)
                lime_vis = lime.visualize_explanation(
                    img_np, lime_exp, num_features=10, hide_rest=False
                )

            lime_col1, lime_col2 = st.columns([1.2, 1])
            with lime_col1:
                st.image(lime_vis, caption="LIME Superpixel Map", use_container_width=True)
            with lime_col2:
                lime_text = ExplanationToText.lime_to_text(lime_exp, disease_name)

                st.markdown(
                    f"""
                    <div style="
                        background-color: #f0fdf4;
                        border-left: 6px solid #16a34a;
                        padding: 15px;
                        border-radius: 8px;
                        font-size: 18px;
                        line-height: 1.5;
                    ">
                        <strong style="color:#15803d; font-size:20px;">üß† Explanation</strong><br><br>
                        {lime_text}
                    </div>
                    """,
                    unsafe_allow_html=True
                )

        st.markdown("</div>", unsafe_allow_html=True)

    # ---------------------------------------------------
    # Treatment Recommendations
    # ---------------------------------------------------
    if image_file:
        st.markdown("<div class='card'>", unsafe_allow_html=True)

        st.markdown("<div class='section-title'>üíä Treatment Recommendations</div>", unsafe_allow_html=True)

        if "GRAD-CAM" not in xai_method:
            cam = gradcam.generate_cam(image_tensor.to(device), class_idx=pred_idx)

        xai_summary = ExplanationToText.gradcam_to_text(cam, disease_name)
        
        if "Gemini" in llm_choice:
            llm = GeminiHandler()
        elif "Llama" in llm_choice:
            llm = Llama2Handler()
        elif "Mistral" in llm_choice:
            llm = MistralHandler()

        with st.spinner("Generating treatment advice..."):
            recommendation = llm.generate_treatment(disease_name, xai_summary)

        st.markdown(recommendation, unsafe_allow_html=True)
        st.markdown("</div>", unsafe_allow_html=True)

def main():
    create_streamlit()

if __name__ == "__main__":
    main()